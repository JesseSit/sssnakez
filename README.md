# sssnakez
	Our testing methodology involves 3 methods. One to generate an array of a desired size populated with random integers from 0 to a desired upper bound, one to record the run time of the merge sort algorithm on a randomly array, and one to call the previous a certain number of times and average the runtimes out. By generating random arrays we are ensuring that we are not getting any special cases just by human selection. The importance of finding the average runtime is so that irregularities are ironed out over thousands of iterations. Such irregularities could be having a lack of computer resources during certain tests. It is important to remember that the results of our tests vary computer to computer due to resources, so you may not have the same results, but the results should be fairly accurate relative to the system.
	To test our algorithm of merge sort against a BigOh classification we used a method of approximating execution time. In this method we took the average runtime of sorting a single element list. This should be constant which would allow us to apply it to the BigOh classifications in order to get an approximate runtime on a specific computer(Jamesâ€™ computer in this case). If you click on the link at the bottom you will be able to view the tables. Notice how O(nlogn) came the closest to the actual outcomes. This difference could be shrunk even more if we extended our algorithm analysis beyond the leading term. This confirmed our hypothesis that merge sort was O(nlogn) after figuring out that it splits the list in half which leads to log(n) and then we run through a loop n times which gives us nlogn. 
  
https://docs.google.com/document/d/1lILfY4lx_79jLt3aA8igrNt5lVmWesSn8cCisKunBJk/pub
